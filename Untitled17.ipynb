{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.datasets import GO_EMOTIONS\n",
    "from flair.models import TARSClassifier\n",
    "from flair.trainers import ModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-14 10:23:25,176 loading file C:\\Users\\STLEE\\.flair\\models\\tars-base-v8.pt\n",
      "{'TREC_6': {'label_dictionary': <flair.data.Dictionary object at 0x0000017989CF6E10>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'DBPedia': {'label_dictionary': <flair.data.Dictionary object at 0x0000017989CF6F98>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'AGNews': {'label_dictionary': <flair.data.Dictionary object at 0x00000179BE5460F0>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'IMDB': {'label_dictionary': <flair.data.Dictionary object at 0x00000179BE546208>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'SST': {'label_dictionary': <flair.data.Dictionary object at 0x00000179BE546320>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'GO_EMOTIONS': {'label_dictionary': <flair.data.Dictionary object at 0x00000179BE546400>, 'multi_label': True, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'NEWS_CATEGORY': {'label_dictionary': <flair.data.Dictionary object at 0x00000179BE546470>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'Amazon': {'label_dictionary': <flair.data.Dictionary object at 0x00000179BE546518>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'Yelp': {'label_dictionary': <flair.data.Dictionary object at 0x0000017989CF6D30>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "tars = TARSClassifier.load(\"tars-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-14 10:23:41,341 https://raw.githubusercontent.com/google-research/google-research/master/goemotions/data/train.tsv not found in cache, downloading to C:\\Users\\STLEE\\AppData\\Local\\Temp\\tmp8610jkw8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3519053B [00:00, 26285054.49B/s]                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-14 10:23:41,513 copying C:\\Users\\STLEE\\AppData\\Local\\Temp\\tmp8610jkw8 to cache at C:\\Users\\STLEE\\.flair\\datasets\\go_emotions\\raw\\train.tsv\n",
      "2021-10-14 10:23:41,521 removing temp file C:\\Users\\STLEE\\AppData\\Local\\Temp\\tmp8610jkw8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-14 10:23:43,123 https://raw.githubusercontent.com/google-research/google-research/master/goemotions/data/test.tsv not found in cache, downloading to C:\\Users\\STLEE\\AppData\\Local\\Temp\\tmpmx_9oerj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "436706B [00:00, 18311649.96B/s]                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-14 10:23:43,183 copying C:\\Users\\STLEE\\AppData\\Local\\Temp\\tmpmx_9oerj to cache at C:\\Users\\STLEE\\.flair\\datasets\\go_emotions\\raw\\test.tsv\n",
      "2021-10-14 10:23:43,189 removing temp file C:\\Users\\STLEE\\AppData\\Local\\Temp\\tmpmx_9oerj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-14 10:23:43,600 https://raw.githubusercontent.com/google-research/google-research/master/goemotions/data/dev.tsv not found in cache, downloading to C:\\Users\\STLEE\\AppData\\Local\\Temp\\tmp7ql97s2p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "439059B [00:00, 16555165.28B/s]                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-14 10:23:43,661 copying C:\\Users\\STLEE\\AppData\\Local\\Temp\\tmp7ql97s2p to cache at C:\\Users\\STLEE\\.flair\\datasets\\go_emotions\\raw\\dev.tsv\n",
      "2021-10-14 10:23:43,667 removing temp file C:\\Users\\STLEE\\AppData\\Local\\Temp\\tmp7ql97s2p\n",
      "2021-10-14 10:23:43,773 Reading data from C:\\Users\\STLEE\\.flair\\datasets\\go_emotions\n",
      "2021-10-14 10:23:43,774 Train: C:\\Users\\STLEE\\.flair\\datasets\\go_emotions\\train.txt\n",
      "2021-10-14 10:23:43,774 Dev: C:\\Users\\STLEE\\.flair\\datasets\\go_emotions\\dev.txt\n",
      "2021-10-14 10:23:43,774 Test: C:\\Users\\STLEE\\.flair\\datasets\\go_emotions\\test.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-14 10:23:45,664 Initialized corpus C:\\Users\\STLEE\\.flair\\datasets\\go_emotions (label type name is 'emotion')\n"
     ]
    }
   ],
   "source": [
    "new_corpus = GO_EMOTIONS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function flair.data.Corpus.get_label_distribution.<locals>.<lambda>()>,\n",
       "            {'NEUTRAL': 14219,\n",
       "             'ANGER': 1567,\n",
       "             'FEAR': 596,\n",
       "             'ANNOYANCE': 2470,\n",
       "             'SURPRISE': 1060,\n",
       "             'GRATITUDE': 2662,\n",
       "             'DESIRE': 641,\n",
       "             'OPTIMISM': 1581,\n",
       "             'ADMIRATION': 4130,\n",
       "             'CONFUSION': 1368,\n",
       "             'AMUSEMENT': 2328,\n",
       "             'APPROVAL': 2939,\n",
       "             'CARING': 1087,\n",
       "             'EMBARRASSMENT': 303,\n",
       "             'REALIZATION': 1110,\n",
       "             'DISAPPOINTMENT': 1269,\n",
       "             'GRIEF': 77,\n",
       "             'SADNESS': 1326,\n",
       "             'CURIOSITY': 2191,\n",
       "             'JOY': 1452,\n",
       "             'LOVE': 2086,\n",
       "             'EXCITEMENT': 853,\n",
       "             'DISAPPROVAL': 2022,\n",
       "             'REMORSE': 545,\n",
       "             'DISGUST': 793,\n",
       "             'RELIEF': 153,\n",
       "             'PRIDE': 111,\n",
       "             'NERVOUSNESS': 164})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_corpus.get_label_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'convert_to_sentence_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14968/2754604861.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhelp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert_to_sentence_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'convert_to_sentence_data' is not defined"
     ]
    }
   ],
   "source": [
    "help(convert_to_sentence_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_type = \"emotion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-14 10:24:14,874 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 43410/43410 [00:12<00:00, 3469.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-14 10:25:11,372 Corpus contains the labels: emotion (#43410)\n",
      "2021-10-14 10:25:11,373 Created (for label 'emotion') Dictionary with 28 tags: NEUTRAL, ANGER, FEAR, ANNOYANCE, SURPRISE, GRATITUDE, DESIRE, OPTIMISM, ADMIRATION, CONFUSION, AMUSEMENT, APPROVAL, CARING, EMBARRASSMENT, REALIZATION, DISAPPOINTMENT, GRIEF, SADNESS, CURIOSITY, JOY, LOVE, EXCITEMENT, DISAPPROVAL, REMORSE, DISGUST, RELIEF, PRIDE, NERVOUSNESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "label_dict = new_corpus.make_label_dictionary(label_type=label_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<flair.data.Dictionary at 0x17989ccfba8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Corpus, Sentence\n",
    "from flair.datasets import SentenceDataset, ClassificationCorpus\n",
    "from flair.trainers import ModelTrainer\n",
    "from flair.models import TARSClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_sentence_data(sentences, labels, task_name):\n",
    "    res = SentenceDataset([\n",
    "      Sentence(s).add_label(task_name, labels[i]) for i, s in enumerate(sentences)\n",
    "          ])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataset consisting of four sentences (2 labeled as \"olahraga\" (sports) and 2 labeled as \"politik\" (politics)\n",
    "train = SentenceDataset(\n",
    "    [\n",
    "        Sentence('Pemilu akan dimulai bulan depan').add_label('olahraga_atau_politik', 'politik'),\n",
    "        Sentence('Perdana menteri mengumumkan dana bantuan banjir besar-besaran').add_label('olahraga_atau_politik', 'politik'),\n",
    "        Sentence('Olimpiade akan diadakan di Tokyo').add_label('olahraga_atau_politik', 'olahraga'),\n",
    "        Sentence('Asian Games 2018 merupakan ajang multi sport yang diadakan di Jakarta').add_label('olahraga_atau_politik', 'olahraga')\n",
    "    ])\n",
    "# test dataset consisting of two sentences (1 labeled as \"olahraga\" and 1 labeled as \"politik\")\n",
    "test = SentenceDataset(\n",
    "    [\n",
    "        Sentence('Para menteri kabinet memainkan permainan yang tidak menyenangkan').add_label('olahraga_atau_politik', 'politik'),\n",
    "        Sentence('Ayo pergi dan bermain basket').add_label('olahraga_atau_politik', 'olahraga')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function add_label in module flair.data:\n",
      "\n",
      "add_label(self, typename: str, value: str, score: float = 1.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Sentence.add_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Sentence in module flair.data:\n",
      "\n",
      "class Sentence(DataPoint)\n",
      " |  Sentence(text: Union[str, List[str]] = None, use_tokenizer: Union[bool, flair.data.Tokenizer] = True, language_code: str = None, start_position: int = None)\n",
      " |  \n",
      " |  A Sentence is a list of tokens and is used to represent a sentence or text fragment.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Sentence\n",
      " |      DataPoint\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __copy__(self)\n",
      " |  \n",
      " |  __getitem__(self, idx: int) -> flair.data.Token\n",
      " |  \n",
      " |  __init__(self, text: Union[str, List[str]] = None, use_tokenizer: Union[bool, flair.data.Tokenizer] = True, language_code: str = None, start_position: int = None)\n",
      " |      Class to hold all meta related to a text (tokens, predictions, language code, ...)\n",
      " |      :param text: original string (sentence), or a list of string tokens (words)\n",
      " |      :param use_tokenizer: a custom tokenizer (default is :class:`SpaceTokenizer`)\n",
      " |          more advanced options are :class:`SegTokTokenizer` to use segtok or :class:`SpacyTokenizer`\n",
      " |          to use Spacy library if available). Check the implementations of abstract class Tokenizer or\n",
      " |          implement your own subclass (if you need it). If instead of providing a Tokenizer, this parameter\n",
      " |          is just set to True (deprecated), :class:`SegtokTokenizer` will be used.\n",
      " |      :param language_code: Language of the sentence\n",
      " |      :param start_position: Start char offset of the sentence in the superordinate document\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |  \n",
      " |  __len__(self) -> int\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __str__(self) -> str\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  add_token(self, token: Union[flair.data.Token, str])\n",
      " |  \n",
      " |  clear_embeddings(self, embedding_names: List[str] = None)\n",
      " |  \n",
      " |  convert_tag_scheme(self, tag_type: str = 'ner', target_scheme: str = 'iob')\n",
      " |  \n",
      " |  get_embedding(self, names: Union[List[str], NoneType] = None) -> <built-in method tensor of type object at 0x00007FFDCE58E620>\n",
      " |  \n",
      " |  get_label_names(self)\n",
      " |  \n",
      " |  get_labels(self, label_type: str = None)\n",
      " |  \n",
      " |  get_language_code(self) -> str\n",
      " |  \n",
      " |  get_spans(self, label_type: Union[str, NoneType] = None, min_score=-1) -> List[flair.data.Span]\n",
      " |  \n",
      " |  get_token(self, token_id: int) -> flair.data.Token\n",
      " |  \n",
      " |  infer_space_after(self)\n",
      " |      Heuristics in case you wish to infer whitespace_after values for tokenized text. This is useful for some old NLP\n",
      " |      tasks (such as CoNLL-03 and CoNLL-2000) that provide only tokenized data with no info of original whitespacing.\n",
      " |      :return:\n",
      " |  \n",
      " |  is_context_set(self) -> bool\n",
      " |      Return True or False depending on whether context is set (for instance in dataloader or elsewhere)\n",
      " |      :return: True if context is set, else False\n",
      " |  \n",
      " |  next_sentence(self)\n",
      " |      Get the next sentence in the document (works only if context is set through dataloader or elsewhere)\n",
      " |      :return: next Sentence in document if set, otherwise None\n",
      " |  \n",
      " |  previous_sentence(self)\n",
      " |      Get the previous sentence in the document (works only if context is set through dataloader or elsewhere)\n",
      " |      :return: previous Sentence in document if set, otherwise None\n",
      " |  \n",
      " |  set_embedding(self, name: str, vector: <built-in method tensor of type object at 0x00007FFDCE58E620>)\n",
      " |  \n",
      " |  to(self, device: str, pin_memory: bool = False)\n",
      " |  \n",
      " |  to_dict(self, tag_type: str = None)\n",
      " |  \n",
      " |  to_original_text(self) -> str\n",
      " |  \n",
      " |  to_plain_string(self)\n",
      " |  \n",
      " |  to_tagged_string(self, main_tag=None) -> str\n",
      " |  \n",
      " |  to_tokenized_string(self) -> str\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  embedding\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from DataPoint:\n",
      " |  \n",
      " |  add_complex_label(self, typename: str, label: flair.data.Label)\n",
      " |  \n",
      " |  add_label(self, typename: str, value: str, score: float = 1.0)\n",
      " |  \n",
      " |  remove_labels(self, typename: str)\n",
      " |  \n",
      " |  set_label(self, typename: str, value: str, score: float = 1.0)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from DataPoint:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  labels\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14968/3990595749.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLabel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data.Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence: \"France is the current world cup winner .\"   [− Tokens: 8  − Sentence-Labels: {'topic': [['sports', 'star'] (1.0)]}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = Sentence('France is the current world cup winner.')\n",
    "\n",
    "# add a label to a sentence\n",
    "sentence.add_label('topic', ['sports','star'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence: \"France is the current world cup winner .\"   [− Tokens: 8  − Sentence-Labels: {'topic': [sports (0)]}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = Sentence('France is the current world cup winner.')\n",
    "sentence.add_label('topic', 'sports',0)\n",
    "#sentence.add_label('topic', 'soccer',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence: \"France is the current world cup winner .\"   [− Tokens: 8  − Sentence-Labels: {'topic': [sports (1.0), soccer (1.0)]}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sent = ['I give my team authority over issues within the department.','I am concerned that my team reach their goal.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [['autonomy and empowerment','creativity'],['autonomy and empowerment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_sentence_data(sentences, labels, task_name):\n",
    "    lst=[]\n",
    "    for i,s in enumerate(sentences):\n",
    "        sentence=Sentence(s)\n",
    "        if len(labels[i])==1:\n",
    "            sentence.add_label(task_name, labels[i][0],1)\n",
    "        else:\n",
    "            for v in labels[i]:\n",
    "                sentence.add_label(task_name, v,1)\n",
    "        lst.append(sentence)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autonomy and empowerment\n",
      "creativity\n"
     ]
    }
   ],
   "source": [
    "for v in train_labels[0]:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<flair.datasets.base.SentenceDataset at 0x1798cfb9588>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SentenceDataset(convert_to_sentence_data(train_sent,train_labels,'class'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_sentence_data(sentences, labels, task_name):\n",
    "    res = SentenceDataset([\n",
    "      Sentence(s).add_label(task_name, labels[i]) for i, s in enumerate(sentences)\n",
    "          ])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(train=train, test=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataset consisting of four sentences (2 labeled as \"olahraga\" (sports) and 2 labeled as \"politik\" (politics)\n",
    "train = SentenceDataset(\n",
    "    [\n",
    "        Sentence('Pemilu akan dimulai bulan depan').add_label('olahraga_atau_politik', 'politik'),\n",
    "        Sentence('Perdana menteri mengumumkan dana bantuan banjir besar-besaran').add_label('olahraga_atau_politik', 'politik'),\n",
    "        Sentence('Olimpiade akan diadakan di Tokyo').add_label('olahraga_atau_politik', 'olahraga'),\n",
    "        Sentence('Asian Games 2018 merupakan ajang multi sport yang diadakan di Jakarta').add_label('olahraga_atau_politik', 'olahraga')\n",
    "    ])\n",
    "# test dataset consisting of two sentences (1 labeled as \"olahraga\" and 1 labeled as \"politik\")\n",
    "test = SentenceDataset(\n",
    "    [\n",
    "        Sentence('Para menteri kabinet memainkan permainan yang tidak menyenangkan').add_label('olahraga_atau_politik', 'politik'),\n",
    "        Sentence('Ayo pergi dan bermain basket').add_label('olahraga_atau_politik', 'olahraga')\n",
    "    ])\n",
    "# make a corpus with train and test split\n",
    "corpus = Corpus(train=train, test=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Corpus in module flair.data:\n",
      "\n",
      "class Corpus(builtins.object)\n",
      " |  Corpus(train: flair.data.FlairDataset, dev: flair.data.FlairDataset = None, test: flair.data.FlairDataset = None, name: str = 'corpus', sample_missing_splits: bool = True)\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, train: flair.data.FlairDataset, dev: flair.data.FlairDataset = None, test: flair.data.FlairDataset = None, name: str = 'corpus', sample_missing_splits: bool = True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __str__(self) -> str\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  downsample(self, percentage: float = 0.1, downsample_train=True, downsample_dev=True, downsample_test=True)\n",
      " |  \n",
      " |  filter_empty_sentences(self)\n",
      " |  \n",
      " |  filter_long_sentences(self, max_charlength: int)\n",
      " |  \n",
      " |  get_all_sentences(self) -> torch.utils.data.dataset.Dataset\n",
      " |  \n",
      " |  get_label_distribution(self)\n",
      " |  \n",
      " |  make_label_dictionary(self, label_type: str) -> flair.data.Dictionary\n",
      " |      Creates a dictionary of all labels assigned to the sentences in the corpus.\n",
      " |      :return: dictionary of labels\n",
      " |  \n",
      " |  make_tag_dictionary(self, tag_type: str) -> flair.data.Dictionary\n",
      " |  \n",
      " |  make_vocab_dictionary(self, max_tokens=-1, min_freq=1) -> flair.data.Dictionary\n",
      " |      Creates a dictionary of all tokens contained in the corpus.\n",
      " |      By defining `max_tokens` you can set the maximum number of tokens that should be contained in the dictionary.\n",
      " |      If there are more than `max_tokens` tokens in the corpus, the most frequent tokens are added first.\n",
      " |      If `min_freq` is set the a value greater than 1 only tokens occurring more than `min_freq` times are considered\n",
      " |      to be added to the dictionary.\n",
      " |      :param max_tokens: the maximum number of tokens that should be added to the dictionary (-1 = take all tokens)\n",
      " |      :param min_freq: a token needs to occur at least `min_freq` times to be added to the dictionary (-1 = there is no limitation)\n",
      " |      :return: dictionary of tokens\n",
      " |  \n",
      " |  obtain_statistics(self, label_type: str = None, pretty_print: bool = True) -> dict\n",
      " |      Print statistics about the class distribution (only labels of sentences are taken into account) and sentence\n",
      " |      sizes.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  dev\n",
      " |  \n",
      " |  test\n",
      " |  \n",
      " |  train\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17 training sentences --10 initiation of structure --7 consideration\n",
    "train_sent = ['I let group members know what is expected of them.', 'I am friendly and approachable.', 'I encourage the use of uniform procedures.', 'I do little things to make it pleasant to be a member of the group.', 'I try out my ideas in the group.', 'I put suggestions made by the group into operation.', 'I make my attitudes clear to the group.', 'I treat all group members as my equals.', 'I decide what shall be done and how it shall be done.', 'I give advance notice of changes.', 'I assign group members to particular tasks.', 'I make sure that my part in the group is understood by the group members.', 'I look out for the personal welfare of group members.', 'I schedule the work to be done.', 'I am willing to make changes.', 'I maintain definite standards of performance.', 'I ask that group members to follow standard rules and regulations.']\n",
    "\n",
    "# each training example is assigned to just 1 of the two labels \n",
    "train_labels = ['initiation of structure.', 'consideration.', 'initiation of structure.', 'consideration.', 'initiation of structure.', 'consideration.', 'initiation of structure.', 'consideration.', 'initiation of structure.', 'consideration.', 'initiation of structure.', 'initiation of structure.', 'consideration.', 'initiation of structure.', 'consideration.', 'initiation of structure.', 'initiation of structure.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_sentence_data(sentences, labels, task_name):\n",
    "    res = SentenceDataset([\n",
    "      Sentence(s).add_label(task_name, labels[i]) for i, s in enumerate(sentences)\n",
    "          ])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.trainers import ModelTrainer\n",
    "from flair.data import Corpus, Sentence\n",
    "from flair.datasets import SentenceDataset, ClassificationCorpus\n",
    "from flair.trainers import ModelTrainer\n",
    "from flair.models import TARSClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-14 22:44:23,215 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 6505.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-14 22:44:23,220 Corpus contains the labels: multi_class_behaviors (#13)\n",
      "2021-10-14 22:44:23,220 Created (for label 'multi_class_behaviors') Dictionary with 2 tags: initiation of structure., consideration.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# use helper to convert to SentenceDataSet\n",
    "train_data = convert_to_sentence_data(train_sent, train_labels, \"multi_class_behaviors\")\n",
    "\n",
    "# make corpus (without setting multi_label = True)\n",
    "corpus = Corpus(train=train_data)\n",
    "label_type=\"multi_class_behaviors\"\n",
    "label_dict = corpus.make_label_dictionary(label_type=label_type)\n",
    "label_dict.multi_label = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict.multi_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-14 22:44:25,863 loading file C:\\Users\\STLEE\\.flair\\models\\tars-base-v8.pt\n",
      "{'TREC_6': {'label_dictionary': <flair.data.Dictionary object at 0x000001428A012B38>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'DBPedia': {'label_dictionary': <flair.data.Dictionary object at 0x000001428A012CC0>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'AGNews': {'label_dictionary': <flair.data.Dictionary object at 0x000001428A012DD8>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'IMDB': {'label_dictionary': <flair.data.Dictionary object at 0x000001428A012EF0>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'SST': {'label_dictionary': <flair.data.Dictionary object at 0x000001428F76C048>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'GO_EMOTIONS': {'label_dictionary': <flair.data.Dictionary object at 0x000001428F76C128>, 'multi_label': True, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'NEWS_CATEGORY': {'label_dictionary': <flair.data.Dictionary object at 0x000001428F76C198>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'Amazon': {'label_dictionary': <flair.data.Dictionary object at 0x000001428F76C240>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'Yelp': {'label_dictionary': <flair.data.Dictionary object at 0x000001428A012A58>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}}\n",
      "2021-10-14 22:44:35,756 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:44:35,758 Model: \"TARSClassifier(\n",
      "  (tars_model): TextClassifier(\n",
      "    (loss_function): CrossEntropyLoss()\n",
      "    (document_embeddings): TransformerDocumentEmbeddings(\n",
      "      (model): BertModel(\n",
      "        (embeddings): BertEmbeddings(\n",
      "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "          (position_embeddings): Embedding(512, 768)\n",
      "          (token_type_embeddings): Embedding(2, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): BertEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (6): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (7): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (8): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (9): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (10): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (11): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (pooler): BertPooler(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (activation): Tanh()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder): Linear(in_features=768, out_features=2, bias=True)\n",
      "  )\n",
      ")\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-14 22:44:35,759 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:44:35,759 Corpus: \"Corpus: 13 train + 2 dev + 2 test sentences\"\n",
      "2021-10-14 22:44:35,760 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:44:35,761 Parameters:\n",
      "2021-10-14 22:44:35,761  - learning_rate: \"0.02\"\n",
      "2021-10-14 22:44:35,762  - mini_batch_size: \"16\"\n",
      "2021-10-14 22:44:35,763  - patience: \"3\"\n",
      "2021-10-14 22:44:35,764  - anneal_factor: \"0.5\"\n",
      "2021-10-14 22:44:35,764  - max_epochs: \"20\"\n",
      "2021-10-14 22:44:35,765  - shuffle: \"True\"\n",
      "2021-10-14 22:44:35,765  - train_with_dev: \"False\"\n",
      "2021-10-14 22:44:35,766  - batch_growth_annealing: \"False\"\n",
      "2021-10-14 22:44:35,766 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:44:35,767 Model training base path: \"resources\\taggerstest\\olahraga_politik\"\n",
      "2021-10-14 22:44:35,767 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:44:35,768 Device: cpu\n",
      "2021-10-14 22:44:35,768 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:44:35,769 Embeddings storage mode: cpu\n",
      "2021-10-14 22:44:35,816 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\STLEE\\AppData\\Roaming\\Python\\Python37\\site-packages\\flair\\trainers\\trainer.py:77: UserWarning: There should be no best model saved at epoch 1 except there is a model from previous trainings in your training folder. All previous best models will be deleted.\n",
      "  \"There should be no best model saved at epoch 1 except there is a model from previous trainings in your training folder. All previous best models will be deleted.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-14 22:44:38,139 epoch 1 - iter 1/1 - loss 0.23100952 - samples/sec: 7.01 - lr: 0.020000\n",
      "2021-10-14 22:44:38,141 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:44:38,141 EPOCH 1 done: loss 0.2310 - lr 0.0200000\n",
      "2021-10-14 22:44:38,299 DEV : loss 0.8089222870767117 - f1-score (micro avg)  0.6667\n",
      "2021-10-14 22:44:38,300 BAD EPOCHS (no improvement): 0\n",
      "2021-10-14 22:44:38,301 saving best model\n",
      "2021-10-14 22:44:39,200 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:44:41,549 epoch 2 - iter 1/1 - loss 0.11622902 - samples/sec: 6.91 - lr: 0.020000\n",
      "2021-10-14 22:44:41,550 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:44:41,551 EPOCH 2 done: loss 0.1162 - lr 0.0200000\n",
      "2021-10-14 22:44:41,717 DEV : loss 0.9702721163630486 - f1-score (micro avg)  0.0\n",
      "2021-10-14 22:44:41,717 BAD EPOCHS (no improvement): 1\n",
      "2021-10-14 22:44:41,719 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:44:44,286 epoch 3 - iter 1/1 - loss 0.12069639 - samples/sec: 6.33 - lr: 0.020000\n",
      "2021-10-14 22:44:44,287 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:44:44,288 EPOCH 3 done: loss 0.1207 - lr 0.0200000\n",
      "2021-10-14 22:44:44,443 DEV : loss 0.731364369392395 - f1-score (micro avg)  0.6667\n",
      "2021-10-14 22:44:44,444 BAD EPOCHS (no improvement): 0\n",
      "2021-10-14 22:44:44,446 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:44:46,888 epoch 4 - iter 1/1 - loss 0.09920505 - samples/sec: 6.64 - lr: 0.020000\n",
      "2021-10-14 22:44:46,889 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:44:46,890 EPOCH 4 done: loss 0.0992 - lr 0.0200000\n",
      "2021-10-14 22:44:47,067 DEV : loss 0.9738779515028 - f1-score (micro avg)  0.0\n",
      "2021-10-14 22:44:47,067 BAD EPOCHS (no improvement): 1\n",
      "2021-10-14 22:44:47,069 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:44:49,449 epoch 5 - iter 1/1 - loss 0.11888035 - samples/sec: 6.82 - lr: 0.020000\n",
      "2021-10-14 22:44:49,450 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:44:49,450 EPOCH 5 done: loss 0.1189 - lr 0.0200000\n",
      "2021-10-14 22:44:49,603 DEV : loss 0.8924640566110611 - f1-score (micro avg)  0.4\n",
      "2021-10-14 22:44:49,603 BAD EPOCHS (no improvement): 2\n",
      "2021-10-14 22:44:49,605 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:44:51,949 epoch 6 - iter 1/1 - loss 0.06635836 - samples/sec: 6.93 - lr: 0.020000\n",
      "2021-10-14 22:44:51,951 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:44:51,952 EPOCH 6 done: loss 0.0664 - lr 0.0200000\n",
      "2021-10-14 22:44:52,105 DEV : loss 0.9335404001176357 - f1-score (micro avg)  0.4\n",
      "2021-10-14 22:44:52,106 BAD EPOCHS (no improvement): 3\n",
      "2021-10-14 22:44:52,108 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:44:54,518 epoch 7 - iter 1/1 - loss 0.04702309 - samples/sec: 6.74 - lr: 0.020000\n",
      "2021-10-14 22:44:54,519 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:44:54,520 EPOCH 7 done: loss 0.0470 - lr 0.0200000\n",
      "2021-10-14 22:44:54,674 DEV : loss 0.8684175424277782 - f1-score (micro avg)  0.4\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-02.\n",
      "2021-10-14 22:44:54,675 BAD EPOCHS (no improvement): 4\n",
      "2021-10-14 22:44:54,677 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:44:57,021 epoch 8 - iter 1/1 - loss 0.04649426 - samples/sec: 6.94 - lr: 0.010000\n",
      "2021-10-14 22:44:57,023 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:44:57,024 EPOCH 8 done: loss 0.0465 - lr 0.0100000\n",
      "2021-10-14 22:44:57,167 DEV : loss 0.7978367395699024 - f1-score (micro avg)  0.5\n",
      "2021-10-14 22:44:57,168 BAD EPOCHS (no improvement): 1\n",
      "2021-10-14 22:44:57,170 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:44:59,490 epoch 9 - iter 1/1 - loss 0.02589882 - samples/sec: 7.04 - lr: 0.010000\n",
      "2021-10-14 22:44:59,492 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:44:59,492 EPOCH 9 done: loss 0.0259 - lr 0.0100000\n",
      "2021-10-14 22:44:59,649 DEV : loss 0.8283350542187691 - f1-score (micro avg)  0.4\n",
      "2021-10-14 22:44:59,650 BAD EPOCHS (no improvement): 2\n",
      "2021-10-14 22:44:59,651 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:45:02,052 epoch 10 - iter 1/1 - loss 0.04091116 - samples/sec: 6.77 - lr: 0.010000\n",
      "2021-10-14 22:45:02,053 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:45:02,054 EPOCH 10 done: loss 0.0409 - lr 0.0100000\n",
      "2021-10-14 22:45:02,214 DEV : loss 0.707200288772583 - f1-score (micro avg)  0.5\n",
      "2021-10-14 22:45:02,215 BAD EPOCHS (no improvement): 3\n",
      "2021-10-14 22:45:02,217 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:45:04,553 epoch 11 - iter 1/1 - loss 0.02587116 - samples/sec: 6.98 - lr: 0.010000\n",
      "2021-10-14 22:45:04,555 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:45:04,556 EPOCH 11 done: loss 0.0259 - lr 0.0100000\n",
      "2021-10-14 22:45:04,710 DEV : loss 0.683375108987093 - f1-score (micro avg)  0.8\n",
      "2021-10-14 22:45:04,710 BAD EPOCHS (no improvement): 0\n",
      "2021-10-14 22:45:04,712 saving best model\n",
      "2021-10-14 22:45:05,678 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:45:08,055 epoch 12 - iter 1/1 - loss 0.02206472 - samples/sec: 6.84 - lr: 0.010000\n",
      "2021-10-14 22:45:08,057 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:45:08,058 EPOCH 12 done: loss 0.0221 - lr 0.0100000\n",
      "2021-10-14 22:45:08,222 DEV : loss 0.5919888131320477 - f1-score (micro avg)  0.5\n",
      "2021-10-14 22:45:08,223 BAD EPOCHS (no improvement): 1\n",
      "2021-10-14 22:45:08,225 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:45:10,597 epoch 13 - iter 1/1 - loss 0.01557189 - samples/sec: 6.85 - lr: 0.010000\n",
      "2021-10-14 22:45:10,598 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:45:10,599 EPOCH 13 done: loss 0.0156 - lr 0.0100000\n",
      "2021-10-14 22:45:10,761 DEV : loss 0.5557532198727131 - f1-score (micro avg)  0.5\n",
      "2021-10-14 22:45:10,762 BAD EPOCHS (no improvement): 2\n",
      "2021-10-14 22:45:10,763 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:45:13,128 epoch 14 - iter 1/1 - loss 0.03281908 - samples/sec: 6.87 - lr: 0.010000\n",
      "2021-10-14 22:45:13,130 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:45:13,130 EPOCH 14 done: loss 0.0328 - lr 0.0100000\n",
      "2021-10-14 22:45:13,296 DEV : loss 0.6005880255252123 - f1-score (micro avg)  0.8\n",
      "2021-10-14 22:45:13,297 BAD EPOCHS (no improvement): 0\n",
      "2021-10-14 22:45:13,298 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:45:15,616 epoch 15 - iter 1/1 - loss 0.00733594 - samples/sec: 7.02 - lr: 0.010000\n",
      "2021-10-14 22:45:15,618 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:45:15,618 EPOCH 15 done: loss 0.0073 - lr 0.0100000\n",
      "2021-10-14 22:45:15,773 DEV : loss 0.4789645029231906 - f1-score (micro avg)  0.6667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-14 22:45:15,774 BAD EPOCHS (no improvement): 1\n",
      "2021-10-14 22:45:15,776 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:45:18,094 epoch 16 - iter 1/1 - loss 0.00942273 - samples/sec: 7.03 - lr: 0.010000\n",
      "2021-10-14 22:45:18,096 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:45:18,097 EPOCH 16 done: loss 0.0094 - lr 0.0100000\n",
      "2021-10-14 22:45:18,256 DEV : loss 0.39267196506261826 - f1-score (micro avg)  0.8\n",
      "2021-10-14 22:45:18,257 BAD EPOCHS (no improvement): 0\n",
      "2021-10-14 22:45:18,259 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:45:20,648 epoch 17 - iter 1/1 - loss 0.00598914 - samples/sec: 6.80 - lr: 0.010000\n",
      "2021-10-14 22:45:20,650 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:45:20,650 EPOCH 17 done: loss 0.0060 - lr 0.0100000\n",
      "2021-10-14 22:45:20,807 DEV : loss 0.6118391323834658 - f1-score (micro avg)  0.8\n",
      "2021-10-14 22:45:20,808 BAD EPOCHS (no improvement): 1\n",
      "2021-10-14 22:45:20,811 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:45:23,283 epoch 18 - iter 1/1 - loss 0.00441792 - samples/sec: 6.57 - lr: 0.010000\n",
      "2021-10-14 22:45:23,285 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:45:23,285 EPOCH 18 done: loss 0.0044 - lr 0.0100000\n",
      "2021-10-14 22:45:23,455 DEV : loss 0.4496829332783818 - f1-score (micro avg)  0.8\n",
      "2021-10-14 22:45:23,456 BAD EPOCHS (no improvement): 2\n",
      "2021-10-14 22:45:23,458 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:45:25,903 epoch 19 - iter 1/1 - loss 0.00476931 - samples/sec: 6.66 - lr: 0.010000\n",
      "2021-10-14 22:45:25,905 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:45:25,906 EPOCH 19 done: loss 0.0048 - lr 0.0100000\n",
      "2021-10-14 22:45:26,065 DEV : loss 0.6347662550397217 - f1-score (micro avg)  0.8\n",
      "2021-10-14 22:45:26,066 BAD EPOCHS (no improvement): 3\n",
      "2021-10-14 22:45:26,068 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:45:28,444 epoch 20 - iter 1/1 - loss 0.00215825 - samples/sec: 6.86 - lr: 0.010000\n",
      "2021-10-14 22:45:28,446 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:45:28,446 EPOCH 20 done: loss 0.0022 - lr 0.0100000\n",
      "2021-10-14 22:45:28,603 DEV : loss 0.4374633012339473 - f1-score (micro avg)  0.8\n",
      "Epoch    20: reducing learning rate of group 0 to 5.0000e-03.\n",
      "2021-10-14 22:45:28,604 BAD EPOCHS (no improvement): 4\n",
      "2021-10-14 22:45:29,299 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 22:45:29,300 loading file resources\\taggerstest\\olahraga_politik\\best-model.pt\n",
      "{'TREC_6': {'label_dictionary': <flair.data.Dictionary object at 0x000001428F76C710>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'DBPedia': {'label_dictionary': <flair.data.Dictionary object at 0x000001428F76C5C0>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'AGNews': {'label_dictionary': <flair.data.Dictionary object at 0x000001428F77EDA0>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'IMDB': {'label_dictionary': <flair.data.Dictionary object at 0x000001428F77E0B8>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'SST': {'label_dictionary': <flair.data.Dictionary object at 0x000001428F77EF60>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'GO_EMOTIONS': {'label_dictionary': <flair.data.Dictionary object at 0x000001428F0E3128>, 'multi_label': True, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'NEWS_CATEGORY': {'label_dictionary': <flair.data.Dictionary object at 0x000001428F0E3A90>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'Amazon': {'label_dictionary': <flair.data.Dictionary object at 0x000001428F0E3BA8>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'Yelp': {'label_dictionary': <flair.data.Dictionary object at 0x000001428F0E3C18>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'multi_class_behaviors': {'label_dictionary': <flair.data.Dictionary object at 0x000001428F1110F0>, 'label_type': 'multi_class_behaviors'}}\n",
      "2021-10-14 22:45:39,344 0.6667\t1.0\t0.8\t0.5\n",
      "2021-10-14 22:45:39,345 \n",
      "Results:\n",
      "- F-score (micro) 0.8\n",
      "- F-score (macro) 0.8333\n",
      "- Accuracy 0.5\n",
      "\n",
      "By class:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "initiation of structure.     0.5000    1.0000    0.6667         1\n",
      "          consideration.     1.0000    1.0000    1.0000         1\n",
      "\n",
      "               micro avg     0.6667    1.0000    0.8000         2\n",
      "               macro avg     0.7500    1.0000    0.8333         2\n",
      "            weighted avg     0.7500    1.0000    0.8333         2\n",
      "             samples avg     0.7500    1.0000    0.8333         2\n",
      "\n",
      "2021-10-14 22:45:39,346 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.8,\n",
       " 'dev_score_history': [0.6666666666666666,\n",
       "  0.0,\n",
       "  0.6666666666666666,\n",
       "  0.0,\n",
       "  0.4,\n",
       "  0.4,\n",
       "  0.4,\n",
       "  0.5,\n",
       "  0.4,\n",
       "  0.5,\n",
       "  0.8,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.8,\n",
       "  0.6666666666666666,\n",
       "  0.8,\n",
       "  0.8,\n",
       "  0.8,\n",
       "  0.8,\n",
       "  0.8],\n",
       " 'train_loss_history': [0.23100952001718375,\n",
       "  0.11622901833974399,\n",
       "  0.12069638761190268,\n",
       "  0.09920504574592297,\n",
       "  0.1188803452711839,\n",
       "  0.0663583565216798,\n",
       "  0.04702308773994446,\n",
       "  0.04649426100345758,\n",
       "  0.025898821365374785,\n",
       "  0.04091115668416023,\n",
       "  0.025871157359618407,\n",
       "  0.022064719635706682,\n",
       "  0.015571892404785523,\n",
       "  0.03281907995159809,\n",
       "  0.007335935217829851,\n",
       "  0.009422727072468171,\n",
       "  0.005989137965326126,\n",
       "  0.004417915756885822,\n",
       "  0.0047693072388378475,\n",
       "  0.002158252253698615],\n",
       " 'dev_loss_history': [0.8089222870767117,\n",
       "  0.9702721163630486,\n",
       "  0.731364369392395,\n",
       "  0.9738779515028,\n",
       "  0.8924640566110611,\n",
       "  0.9335404001176357,\n",
       "  0.8684175424277782,\n",
       "  0.7978367395699024,\n",
       "  0.8283350542187691,\n",
       "  0.707200288772583,\n",
       "  0.683375108987093,\n",
       "  0.5919888131320477,\n",
       "  0.5557532198727131,\n",
       "  0.6005880255252123,\n",
       "  0.4789645029231906,\n",
       "  0.39267196506261826,\n",
       "  0.6118391323834658,\n",
       "  0.4496829332783818,\n",
       "  0.6347662550397217,\n",
       "  0.4374633012339473]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. load base TARS\n",
    "tars = TARSClassifier.load('tars-base')\n",
    "# 2. make the model aware of the desired set of labels from the new corpus\n",
    "tars.add_and_switch_to_new_task(task_name = \"multi_class_behaviors\", \n",
    "                                label_dictionary=label_dict,\n",
    "                               label_type=label_type)\n",
    "# 3. initialize the text classifier trainer with your corpus\n",
    "trainer = ModelTrainer(tars, corpus)\n",
    "# 4. train model\n",
    "trainer.train(base_path='resources/taggerstest/olahraga_politik', # path to store the model artifacts\n",
    "              learning_rate=0.02, \n",
    "              mini_batch_size=16,\n",
    "              mini_batch_chunk_size=4,\n",
    "              max_epochs=20,\n",
    "              embeddings_storage_mode=\"cpu\",\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-14 22:51:51,394 loading file resources/taggerstest/olahraga_politik/best-model.pt\n",
      "{'TREC_6': {'label_dictionary': <flair.data.Dictionary object at 0x000001433EBE8CC0>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'DBPedia': {'label_dictionary': <flair.data.Dictionary object at 0x000001433EBE80F0>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'AGNews': {'label_dictionary': <flair.data.Dictionary object at 0x000001433EBE8B38>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'IMDB': {'label_dictionary': <flair.data.Dictionary object at 0x000001433EBE8630>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'SST': {'label_dictionary': <flair.data.Dictionary object at 0x000001433EBE8940>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'GO_EMOTIONS': {'label_dictionary': <flair.data.Dictionary object at 0x000001433EBE8E10>, 'multi_label': True, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'NEWS_CATEGORY': {'label_dictionary': <flair.data.Dictionary object at 0x000001433EBE8D30>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'Amazon': {'label_dictionary': <flair.data.Dictionary object at 0x000001433EBE8128>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'Yelp': {'label_dictionary': <flair.data.Dictionary object at 0x000001433EBE81D0>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'multi_class_behaviors': {'label_dictionary': <flair.data.Dictionary object at 0x0000014289F690B8>, 'label_type': 'multi_class_behaviors'}}\n"
     ]
    }
   ],
   "source": [
    "ft_tars = TARSClassifier.load('resources/taggerstest/olahraga_politik/best-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = Sentence('I let group members know what is expected of them.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_tars.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = ['I am friendly and approachable.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [Sentence(s) for s in test_data]\n",
    "ft_tars.predict(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sentence: \"I am friendly and approachable .\"   [− Tokens: 6  − Sentence-Labels: {'multi_class_behaviors': [initiation of structure. (0.5571), consideration. (0.6574)]}]]\n"
     ]
    }
   ],
   "source": [
    "print(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-14 15:16:59,703 https://cogcomp.seas.upenn.edu/Data/QA/QC/train_5500.label not found in cache, downloading to C:\\Users\\STLEE\\AppData\\Local\\Temp\\tmpdh0thnmv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 335858/335858 [00:00<00:00, 346303.74B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-14 15:17:01,405 copying C:\\Users\\STLEE\\AppData\\Local\\Temp\\tmpdh0thnmv to cache at C:\\Users\\STLEE\\.flair\\datasets\\trec_6\\original\\train_5500.label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-14 15:17:01,958 removing temp file C:\\Users\\STLEE\\AppData\\Local\\Temp\\tmpdh0thnmv\n",
      "2021-10-14 15:17:02,744 https://cogcomp.seas.upenn.edu/Data/QA/QC/TREC_10.label not found in cache, downloading to C:\\Users\\STLEE\\AppData\\Local\\Temp\\tmp8s7hjdea\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 23354/23354 [00:00<00:00, 93377.00B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-14 15:17:03,702 copying C:\\Users\\STLEE\\AppData\\Local\\Temp\\tmp8s7hjdea to cache at C:\\Users\\STLEE\\.flair\\datasets\\trec_6\\original\\TREC_10.label\n",
      "2021-10-14 15:17:03,840 removing temp file C:\\Users\\STLEE\\AppData\\Local\\Temp\\tmp8s7hjdea\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-14 15:17:04,168 Reading data from C:\\Users\\STLEE\\.flair\\datasets\\trec_6\n",
      "2021-10-14 15:17:04,168 Train: C:\\Users\\STLEE\\.flair\\datasets\\trec_6\\train.txt\n",
      "2021-10-14 15:17:04,168 Dev: None\n",
      "2021-10-14 15:17:04,185 Test: C:\\Users\\STLEE\\.flair\\datasets\\trec_6\\test.txt\n",
      "2021-10-14 15:17:27,627 Initialized corpus C:\\Users\\STLEE\\.flair\\datasets\\trec_6 (label type name is 'question_class')\n",
      "2021-10-14 15:17:27,627 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 4907/4907 [00:00<00:00, 19957.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-14 15:17:27,882 Corpus contains the labels: question_class (#4907)\n",
      "2021-10-14 15:17:27,883 Created (for label 'question_class') Dictionary with 6 tags: question about description, question about entity, question about abbreviation, question about person, question about number, question about location\n",
      "2021-10-14 15:17:27,884 loading file C:\\Users\\STLEE\\.flair\\models\\tars-base-v8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TREC_6': {'label_dictionary': <flair.data.Dictionary object at 0x00000179933D1A20>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'DBPedia': {'label_dictionary': <flair.data.Dictionary object at 0x00000179933D1BA8>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'AGNews': {'label_dictionary': <flair.data.Dictionary object at 0x00000179933D1CC0>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'IMDB': {'label_dictionary': <flair.data.Dictionary object at 0x00000179933D1DD8>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'SST': {'label_dictionary': <flair.data.Dictionary object at 0x00000179933D1EF0>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'GO_EMOTIONS': {'label_dictionary': <flair.data.Dictionary object at 0x00000179933D1FD0>, 'multi_label': True, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'NEWS_CATEGORY': {'label_dictionary': <flair.data.Dictionary object at 0x00000179AE8FC080>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'Amazon': {'label_dictionary': <flair.data.Dictionary object at 0x00000179AE8FC128>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}, 'Yelp': {'label_dictionary': <flair.data.Dictionary object at 0x00000179933D1940>, 'multi_label': False, 'multi_label_threshold': 0.5, 'label_type': None, 'beta': 1.0}}\n",
      "2021-10-14 15:17:37,531 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 15:17:37,549 Model: \"TARSClassifier(\n",
      "  (tars_model): TextClassifier(\n",
      "    (loss_function): CrossEntropyLoss()\n",
      "    (document_embeddings): TransformerDocumentEmbeddings(\n",
      "      (model): BertModel(\n",
      "        (embeddings): BertEmbeddings(\n",
      "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "          (position_embeddings): Embedding(512, 768)\n",
      "          (token_type_embeddings): Embedding(2, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): BertEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (6): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (7): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (8): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (9): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (10): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (11): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (pooler): BertPooler(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (activation): Tanh()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder): Linear(in_features=768, out_features=2, bias=True)\n",
      "  )\n",
      ")\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-14 15:17:37,555 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 15:17:37,556 Corpus: \"Corpus: 4907 train + 545 dev + 500 test sentences\"\n",
      "2021-10-14 15:17:37,557 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 15:17:37,558 Parameters:\n",
      "2021-10-14 15:17:37,559  - learning_rate: \"0.02\"\n",
      "2021-10-14 15:17:37,560  - mini_batch_size: \"16\"\n",
      "2021-10-14 15:17:37,561  - patience: \"3\"\n",
      "2021-10-14 15:17:37,562  - anneal_factor: \"0.5\"\n",
      "2021-10-14 15:17:37,563  - max_epochs: \"1\"\n",
      "2021-10-14 15:17:37,563  - shuffle: \"True\"\n",
      "2021-10-14 15:17:37,564  - train_with_dev: \"False\"\n",
      "2021-10-14 15:17:37,565  - batch_growth_annealing: \"False\"\n",
      "2021-10-14 15:17:37,566 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 15:17:37,566 Model training base path: \"resources\\taggers\\trec\"\n",
      "2021-10-14 15:17:37,567 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 15:17:37,568 Device: cpu\n",
      "2021-10-14 15:17:37,569 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 15:17:37,570 Embeddings storage mode: cpu\n",
      "2021-10-14 15:17:37,573 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 15:19:33,174 epoch 1 - iter 30/307 - loss 0.00735817 - samples/sec: 4.16 - lr: 0.020000\n",
      "2021-10-14 15:21:49,225 epoch 1 - iter 60/307 - loss 0.00717103 - samples/sec: 3.53 - lr: 0.020000\n",
      "2021-10-14 15:24:04,162 epoch 1 - iter 90/307 - loss 0.00710932 - samples/sec: 3.56 - lr: 0.020000\n",
      "2021-10-14 15:24:08,062 ----------------------------------------------------------------------------------------------------\n",
      "2021-10-14 15:24:08,063 Exiting from training early.\n",
      "2021-10-14 15:24:08,064 Saving model ...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\caffe2\\serialize\\inline_container.cc:274] . unexpected pos 324629312 vs 324629200",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\flair\\trainers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, base_path, learning_rate, mini_batch_size, mini_batch_chunk_size, max_epochs, scheduler, cycle_momentum, anneal_factor, patience, initial_extra_patience, min_learning_rate, train_with_dev, train_with_test, monitor_train, monitor_test, embeddings_storage_mode, checkpoint, save_final_model, anneal_with_restarts, anneal_with_prestarts, anneal_against_dev_loss, batch_growth_annealing, shuffle, param_selection_mode, write_weights, num_workers, sampler, use_amp, amp_opt_level, eval_on_train_fraction, eval_on_train_shuffle, save_model_each_k_epochs, main_evaluation_metric, tensorboard_comment, save_best_checkpoints, use_swa, use_final_model_for_eval, gold_label_dictionary_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m    432\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m                             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m                         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    371\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 372\u001b[1;33m                 \u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    373\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[0mnum_bytes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melement_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m         \u001b[0mzip_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14968/952951446.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m               \u001b[0mmini_batch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m               \u001b[0mmini_batch_chunk_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# optionally set this if transformer is too much for your machine\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m               \u001b[0mmax_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# terminate after 10 epochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m               )\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\flair\\trainers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, base_path, learning_rate, mini_batch_size, mini_batch_chunk_size, max_epochs, scheduler, cycle_momentum, anneal_factor, patience, initial_extra_patience, min_learning_rate, train_with_dev, train_with_test, monitor_train, monitor_test, embeddings_storage_mode, checkpoint, save_final_model, anneal_with_restarts, anneal_with_prestarts, anneal_against_dev_loss, batch_growth_annealing, shuffle, param_selection_mode, write_weights, num_workers, sampler, use_amp, amp_opt_level, eval_on_train_fraction, eval_on_train_shuffle, save_model_each_k_epochs, main_evaluation_metric, tensorboard_comment, save_best_checkpoints, use_swa, use_final_model_for_eval, gold_label_dictionary_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m    699\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mparam_selection_mode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Saving model ...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_path\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m\"final-model.pt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\flair\\nn\\model.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, model_file)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mmodel_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    371\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m                 \u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\caffe2\\serialize\\inline_container.cc:274] . unexpected pos 324629312 vs 324629200"
     ]
    }
   ],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import TREC_6\n",
    "from flair.models import TARSClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "# 1. define label names in natural language since some datasets come with cryptic set of labels\n",
    "label_name_map = {'ENTY': 'question about entity',\n",
    "                  'DESC': 'question about description',\n",
    "                  'ABBR': 'question about abbreviation',\n",
    "                  'HUM': 'question about person',\n",
    "                  'NUM': 'question about number',\n",
    "                  'LOC': 'question about location'\n",
    "                  }\n",
    "\n",
    "# 2. get the corpus\n",
    "corpus: Corpus = TREC_6(label_name_map=label_name_map)\n",
    "\n",
    "# 3. what label do you want to predict?\n",
    "label_type = 'question_class'\n",
    "\n",
    "# 4. make a label dictionary\n",
    "label_dict = corpus.make_label_dictionary(label_type=label_type)\n",
    "\n",
    "# 5. start from our existing TARS base model for English\n",
    "tars = TARSClassifier.load(\"tars-base\")\n",
    "\n",
    "# 5a: alternatively, comment out previous line and comment in next line to train a new TARS model from scratch instead\n",
    "# tars = TARSClassifier(embeddings=\"bert-base-uncased\")\n",
    "\n",
    "# 6. switch to a new task (TARS can do multiple tasks so you must define one)\n",
    "tars.add_and_switch_to_new_task(task_name=\"question classification\",\n",
    "                                label_dictionary=label_dict,\n",
    "                                label_type=label_type,\n",
    "                                )\n",
    "\n",
    "# 7. initialize the text classifier trainer\n",
    "trainer = ModelTrainer(tars, corpus)\n",
    "\n",
    "# 8. start the training\n",
    "trainer.train(base_path='resources/taggers/trec',  # path to store the model artifacts\n",
    "              learning_rate=0.02,  # use very small learning rate\n",
    "              mini_batch_size=16,\n",
    "              mini_batch_chunk_size=4,  # optionally set this if transformer is too much for your machine\n",
    "              max_epochs=1,  # terminate after 10 epochs\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
